<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>【笔记】机器学习第七章 基于最小二乘的分类</title>
    <link href="/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%83%E7%AB%A0-%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <url>/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%83%E7%AB%A0-%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E7%9A%84%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="第七章-基于最小二乘法的分类"><a href="#第七章-基于最小二乘法的分类" class="headerlink" title="第七章 基于最小二乘法的分类"></a>第七章 基于最小二乘法的分类</h1><p>以<strong>2类别</strong>分类问题为例，研究机器学习在处理分类问题时的方法。</p><p>对于2分类问题，分类器的学习可以近似地定义为取值为 $+1$ 或 $-1$ 的二值函数问题。</p><h2 id="7-1-最小二乘分类"><a href="#7-1-最小二乘分类" class="headerlink" title="7.1 最小二乘分类"></a>7.1 最小二乘分类</h2><p>对于最简单的这样的二值函数问题，可以用最小二乘法。其过程与回归问题相同：</p><script type="math/tex; mode=display">\hat{\mathbf{\theta}} = \arg \min \limits_{\mathbf{\theta}} \frac{1}{2} \sum_{i=1}^n (f_{\mathbf{\theta}}(\mathbf{x}_i) - y_i)^2</script><p>模型输入 $x$ 所对应的类别 $y$ 的预测值 $\hat{y}$ 是由学习后输出结果的符号决定的：</p><script type="math/tex; mode=display">\hat{y} = sign(f_{\hat{\mathbf{\theta}}}(\mathbf{x})) = \left\{\begin{aligned}+1 \qquad f_{\hat{\mathbf{\theta}}}(\mathbf{x}) > 0 \\0 \qquad f_{\hat{\mathbf{\theta}}}(\mathbf{x}) = 0 \\-1 \qquad f_{\hat{\mathbf{\theta}}}(\mathbf{x}) < 0\end{aligned}\right.</script><p>其中 $f_{\hat{\mathbf{\theta}}}(\mathbf{x}) = 0$ 是指小概率事件。</p><p>当正负两类样本的模式都服从<strong>协方差矩阵相同的高斯分布</strong>时，分类器可以获得最佳的泛化能力。</p><h2 id="7-2-0-1-损失和间隔"><a href="#7-2-0-1-损失和间隔" class="headerlink" title="7.2 $0/1$ 损失和间隔"></a>7.2 $0/1$ 损失和间隔</h2><p>由于我们只用了输出结果的符号，而函数值本身并不重要，因此，如果应用 $0/1$ 损失可以获得更好的效果：</p><script type="math/tex; mode=display">\frac{1}{2} (1 - sign (f_{\mathbf{\theta}}(\mathbf{x})y))</script><p>即：</p><script type="math/tex; mode=display">\delta (sign(f_{\mathbf{\theta}}(\mathbf{x})) \neq y) =\left\{\begin{aligned}1 \qquad (sign(f_{\mathbf{\theta}}(\mathbf{x})) \neq y) \\0 \qquad (sign(f_{\mathbf{\theta}}(\mathbf{x})) = y)\end{aligned}\right.</script><p>当分类错误的时候，函数结果为 $1$；当分类正确的时候，函数结果为 $0$。这样，$0/1$ 损失就可以用来对错误分类的样本个数进行统计。</p><p>我们将 $m = f_{\mathbf{\theta}}(\mathbf{x})y$ 称为训练样本的间隔。当 $m &gt; 0$ 的时候，损失为 $0$（即分类正确）；$m \leq 0$ 的时候，损失为 $1$（即分类错误）。</p><p>$0/1$ 损失并不依赖 $m$ 的大小，只依赖于其正负，所以，$m$ 应尽可能取得更大的值，使得学习结果更加稳定。</p><p>然而 $0/1$ 损失很难计算，也就难以用它对学习能力和泛化性做出评估。实际中通常使用 $\ell_2$ 损失作为它的代理损失进行计算。</p><p>由于 $y \in {+1, -1}$，所以 $y^2 = 1$ 和 $1/y = y$ 成立，这样 $\ell_2$ 损失就可以变形成：</p><script type="math/tex; mode=display">r^2 = (y - f_{\mathbf{\theta}}(\mathbf{x}))^2 = (1 - f_{\mathbf{\theta}}(\mathbf{x})y)^2 = (1 - m)^2</script><p>其中，$m= f_{\mathbf{\theta}}(\mathbf{x})y$ 是训练样本的间隔。</p><p>$\ell_2$ 损失具有值越小越容易学习的特点；但是当 $m &gt;1$ 的时候，$\ell_2$ 损失有倾向正的可能，因此最终不会得到最小二乘的最优解。</p><p>较好的代理损失应当是间隔 $m$ 的单调非增函数，且当间隔 $m=0$ 时微分为负。后几章将会介绍其他较好的代理损失函数。</p><h2 id="7-3-多类别的情形"><a href="#7-3-多类别的情形" class="headerlink" title="7.3 多类别的情形"></a>7.3 多类别的情形</h2><p>以上两节我们介绍了2类别分类问题的最简单学习方法——基于最小二乘的分类。但是实际中分类的类别往往不止两个。这里，我们可以用两种方法扩展2类别分类方法为多类别分类方法。</p><ul><li>需要分类的类别有 $y = 1, 2, 3, …, c$</li></ul><h3 id="7-3-1-一对多法"><a href="#7-3-1-一对多法" class="headerlink" title="7.3.1 一对多法"></a>7.3.1 一对多法</h3><p>首先，将类别1设置为 $+1$，而其他所有类别， 即 $y = 2, 3, …, c$ 全部设置为 $-1$。这样，我们就把多类别分类转换化了2类别分类。训练得到第一个识别函数 $\hat{f}_1$。</p><p>这样将每一个分类转化为2分类问题并求其识别函数就可以得到 $\hat{f}_1, \hat{f}_2, …, \hat{f}_c$。</p><p>总的分类函数就是：</p><script type="math/tex; mode=display">\hat{y} = \arg \max \limits_{y = 1, 2, ..., c} \hat{f}_y(\mathbf{x})</script><p>即在一对多法中，从各个2类别的分类问题中训练得到的 $c$ 个识别函数 $\hat{f}_1, \hat{f}_2, …, \hat{f}_c$ 的输出，表示的是测试样本 $\mathbf{x}$ 属于类别 $y$ 的概率，概率最大的那个一就是测试样本 $\mathbf{x}$ 所属的类别。</p><h3 id="7-3-2-一对一法"><a href="#7-3-2-一对一法" class="headerlink" title="7.3.2 一对一法"></a>7.3.2 一对一法</h3><p>首先，对于所有的 $y, y’ = 1, 2, …, c$ ，在任意两类之间训练一个分类器，属于类别 $y$ 的标签为 $+1$，$y’$ 的标签为 $-1$。</p><p>这样就得到了 $c(c-1)/2$ 个识别函数 ${\hat{f}_{y, y’}(\mathbf{x})}$，用这些识别函数对样本进行预测，再用投票法决定最终的类别：</p><script type="math/tex; mode=display">sign(\hat{f}_{y, y'}(\mathbf{x})) = \left\{\begin{aligned}+ 1 \quad 投票给类别y \\- 1 \quad 投票给类别y'\end{aligned}\right.</script><p>的票最多的类别就是样本 $\mathbf{x}$ 所属的类别。</p>]]></content>
    
    
    
    <tags>
      
      <tag>MachineLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【笔记】机器学习第四章 带有约束条件的最小二乘法</title>
    <link href="/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%B8%A6%E6%9C%89%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6%E7%9A%84%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/"/>
    <url>/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%B8%A6%E6%9C%89%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6%E7%9A%84%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="第四章-带有约束条件的最小二乘法"><a href="#第四章-带有约束条件的最小二乘法" class="headerlink" title="第四章 带有约束条件的最小二乘法"></a>第四章 带有约束条件的最小二乘法</h1><p>单纯的最小二乘法对于包含噪声的学习过程经常有过拟合的弱点，这是由于学习模型对于样本而言过度复杂而造成的。为了解决这个问题，本章介绍能够控制模型复杂程度的、带有约束条件的最小二乘学习法。</p><h2 id="4-1-部分空间约束的最小二乘学习法"><a href="#4-1-部分空间约束的最小二乘学习法" class="headerlink" title="4.1 部分空间约束的最小二乘学习法"></a>4.1 部分空间约束的最小二乘学习法</h2><ul><li>有参数的线性模型的参数可以自由设置，所以其参数空间是 <strong>全体参数空间</strong>：</li></ul><script type="math/tex; mode=display">f_{\theta}(\mathbf{x}) = \sum_{j=1}^b \theta_i \phi_i (\mathbf{x}) = \mathbf{\theta}^\top \mathbf{\phi}(\mathbf{x})</script><p>要约束学习模型的复杂程度，就是要给这些参数一个约束（乘一个系数），以降低其影响，防止过拟合现象：</p><script type="math/tex; mode=display">\min\limits_{\mathbf{\theta}} J_{LS} (\mathbf{\theta}), \quad constraint: \mathbf{P\theta} = \mathbf{\theta}</script><p>式中，$\mathbf{P}$ 是满足 $\mathbf{P}^2 = \mathbf{P}$ 和 $\mathbf{P}^\top = \mathbf{P}$ 的 $b \times b$ 维矩阵，表示矩阵 $\mathbf{P}$ 的值域 $\Re({\mathbf{P})}$ 的正交投影矩阵。通过这个约束条件 $\mathbf{P}$ 矩阵，参数 $\mathbf{theta}$ 就不会偏移到值域 $\Re({\mathbf{P})}$ 的范围之外了。</p><p>部分空间约束的最小二乘学习法的解 $\hat{\mathbf{\theta}}$ 可以通过将设计矩阵 $\mathbf{\Phi}$ 置换为 $\mathbf{\Phi P}$ 的方式求得：</p><script type="math/tex; mode=display">\hat{\mathbf{\theta}} = (\mathbf{\Phi P})^\dagger \mathbf{y}</script><p>正交投影矩阵 $\mathbf{P}$ 可以手动设置，也可以使用 <strong>主成分分析法</strong> 基于数据进行设置。</p><h2 id="4-2-ell-2-约束的最小二乘学习法"><a href="#4-2-ell-2-约束的最小二乘学习法" class="headerlink" title="4.2 $\ell_2$ 约束的最小二乘学习法"></a>4.2 $\ell_2$ 约束的最小二乘学习法</h2><h3 id="4-2-1-部分空间约束的最小二乘学习法的局限"><a href="#4-2-1-部分空间约束的最小二乘学习法的局限" class="headerlink" title="4.2.1 部分空间约束的最小二乘学习法的局限"></a>4.2.1 部分空间约束的最小二乘学习法的局限</h3><p>部分空间约束的最小二乘学习法在设置正交投影矩阵 $\mathbf{P}$ 时有很大的自由度。一方面，这可以使得研究人员可以根据自己的需要对约束进行自由的定义；另一方面，这也使得在实际应用中很难确定具体的值。</p><h3 id="4-2-2-ell-2-约束的最小二乘学习法的定义"><a href="#4-2-2-ell-2-约束的最小二乘学习法的定义" class="headerlink" title="4.2.2 $\ell_2$ 约束的最小二乘学习法的定义"></a>4.2.2 $\ell_2$ 约束的最小二乘学习法的定义</h3><p>$\ell_2$ 约束的最小二乘学习法利用拉格朗日对偶问题的求解法，在以参数空间的原点为圆心、半径一定的超球内寻找最优解：</p><script type="math/tex; mode=display">\min \limits_\mathbf{\theta} J_{LS} (\mathbf{\theta}), \quad constraint: \left \| \mathbf{\theta} \right \|^2 \leq R</script><h3 id="4-2-3-拉格朗日对偶问题"><a href="#4-2-3-拉格朗日对偶问题" class="headerlink" title="4.2.3 拉格朗日对偶问题*"></a>4.2.3 拉格朗日对偶问题*</h3><p>可微凸函数 $f$ 和 $\mathbf{g}$ 的约束条件的最小化问题</p><script type="math/tex; mode=display">\min \limits_\mathbf{t} f(\mathbf{t}), \quad constraint: \mathbf{g}(\mathbf{t}) \leq 0</script><p>的拉格朗日对偶问题，可以使用拉格朗日乘子</p><script type="math/tex; mode=display">\mathbf{\lambda} = (\lambda_1, ..., \lambda_p)^\top</script><p>和拉格朗日函数</p><script type="math/tex; mode=display">L(\mathbf{t}, \mathbf{\lambda}) = f(\mathbf{t}) + \mathbf{\lambda}^\top \mathbf{g}(\mathbf{t})</script><p>采用以下方式进行定义：</p><script type="math/tex; mode=display">\max \limits_\mathbf{\lambda} \inf \limits_t L(\mathbf{t}, \mathbf{\lambda}), \quad constraint: \mathbf{\lambda}\geq 0</script><p>拉格朗日对偶问题的 $\mathbf{t}$ 的解与原来问题的解是一致的。</p><h3 id="4-2-4-ell-2-约束最小二乘学习法"><a href="#4-2-4-ell-2-约束最小二乘学习法" class="headerlink" title="4.2.4 $\ell_2$ 约束最小二乘学习法"></a>4.2.4 $\ell_2$ 约束最小二乘学习法</h3><p>根据$\ell_2$ 约束最小二乘学习法的定义和拉格朗日对偶问题解法，可以得到：</p><script type="math/tex; mode=display">\max \limits_\lambda \min \limits_{\mathbf{\theta}} [J_{LS}(\mathbf{\theta}) + \frac{\lambda}{2}(\left \| \mathbf{\theta} \right \|^2 - R)], \quad constraint: \lambda \geq 0</script><p>拉格朗日乘子  的解由圆的半径  决定或直接指定；在直接指定时，$\ell_2$ 约束的最小二乘学习法的解 $\hat{\mathbf{\theta}}$ 就是：</p><script type="math/tex; mode=display">\hat{\mathbf{\theta}} = \arg \min \limits_{\mathbf{\theta}} [J_{LS}(\mathbf{\theta}) + \frac{\lambda}{2}\left \| \mathbf{\theta}  \right \|^2]</script><p>式中，第一项 $J_{LS}(\mathbf{\theta})$ 表示训练样本的拟合程度，通过与第二项的 $\frac{\lambda}{2}\left || \mathbf{\theta} \right ||^2$ 相结合得到最小值，以防止训练样本过拟合；$\left || \mathbf{\theta} \right ||^2$ 为正则项，$\lambda$ 为正则化系数。</p><p>应用最小二乘法的定义，令：</p><script type="math/tex; mode=display">\nabla_{\theta} J_{LS}(\mathbf{\theta})= 0</script><p>则 $\ell_2$ 约束的最小二乘学习法的解 $\hat{\mathbf{\theta}}$ 可以表示为：</p><script type="math/tex; mode=display">\hat{\mathbf{\theta}} = (\mathbf{\Phi}^\top \mathbf{\Phi} + \lambda \mathbf{I})^{-1} \mathbf{\Phi}^\top \mathbf{y}</script><p>通过将矩阵 $\mathbf{\Phi}^\top \mathbf{\Phi}$ 与 $\lambda \mathbf{I}$ 相加提高其正则性，进而就可以更稳定地进行矩阵求逆。</p><ul><li>设计矩阵 $\mathbf{\Phi}$ 的奇异值分解为：</li></ul><script type="math/tex; mode=display">\mathbf{\Phi} = \sum_{k=1}^{\min(n,b)} \kappa_k \psi_k \varphi_k^\top</script><p>则$\ell_2$ 约束的最小二乘学习法的解  就可以表示为：</p><script type="math/tex; mode=display">\hat{\mathbf{\theta}} = \sum_{k=1}^{\min(n, b)} \frac{\kappa_k}{\kappa_k^2 + \lambda} \mathbf{\psi}_k^\top \mathbf{y} \mathbf{\varphi}_k</script><p>当 $\lambda = 0$ 的时候，$\ell_2$ 约束的最小二乘学习法与一般的最小二乘法相同；此时，如果 $\mathbf{\Phi}$ 包含很小的奇异值 $\kappa_k$（计算条件很恶劣）的时候，$\frac{\kappa_k}{\kappa_k^2} = \frac{1}{\kappa_k}$ 就非常大，训练输出向量 $\mathbf{y}$ 就会包含很多噪声。</p><p>在分母中加入正常数 $\lambda$，就可以避免 $\frac{\kappa_k}{\kappa_k^2 + \lambda}$ 过大，从而防止过拟合。</p><h3 id="4-2-5-一般-ell-2-约束的最小二乘学习法"><a href="#4-2-5-一般-ell-2-约束的最小二乘学习法" class="headerlink" title="4.2.5 一般 $\ell_2$ 约束的最小二乘学习法"></a>4.2.5 一般 $\ell_2$ 约束的最小二乘学习法</h3><p>对于 $\ell_2$ 约束的最小二乘学习法的更一般的表述为：</p><script type="math/tex; mode=display">\min \limits_\mathbf{\theta} J_{LS} (\mathbf{\theta}), \quad constraint: \mathbf{\theta}^\top \mathbf{G} \mathbf{\theta} \leq R</script><p>式中，$\mathbf{G}$ 是 $b \times b$ 的正则化矩阵。</p><p>当 $\mathbf{G}$ 是对称正定矩阵的时候，可以把数据限制在椭圆形状的数据区域内。</p><p>一般 $\ell_2$ 约束的最小二乘学习法的解 $\hat{\mathbf{\theta}}$ 的求解过程与上述 $\ell_2$ 约束的最小二乘学习法大致相同：</p><script type="math/tex; mode=display">\hat{\mathbf{\theta}} = (\mathbf{\Phi}^\top \mathbf{\Phi} + \lambda \mathbf{G})^{-1} \mathbf{\Phi}^\top \mathbf{y}</script><h2 id="4-3-模型选择"><a href="#4-3-模型选择" class="headerlink" title="4.3 模型选择"></a>4.3 模型选择</h2><p>无论是 <strong>部分空间约束的最小二乘学习法</strong> 还是 <strong>$\ell_2$ 约束的最小二乘学习法</strong>，它们对过拟合现象的缓解都依赖于参数的选择。为了获得更好的结果，选择合适的参数至关重要；同时，使用线性模型时基函数的种类和数量的选择，使用核模型时核函数的种类和参数的选择，也都会直接影响学习的效果。</p><h3 id="4-3-1-模型选择"><a href="#4-3-1-模型选择" class="headerlink" title="4.3.1 模型选择"></a>4.3.1 模型选择</h3><p>通过采用不同的输入训练样本来决定机器学习算法中包含的各个参数值。流程如下：</p><ol><li><p>准备模型的候选 $M_1, M_2, …, M_k$。</p></li><li><p>对各个模型候选 $M_1, M_2, …, M_k$ 求解其学习结果 $f^{(1)}, f^{(2)}, …, f^{(k)}$。</p></li><li><p>对各学习结果 $f^{(1)}, f^{(2)}, …, f^{(k)}$ 的泛化误差 $G^{(1)}, G^{(2)}, …, G^{(k)}$ 进行评估。</p></li><li><p>选择泛化误差 $G^{(1)}, G^{(2)}, …, G^{(k)}$ 最小的模型为最终模型。</p></li></ol><p>其中最为重要的是第 3 步：泛化误差的评估。</p><h3 id="4-3-2-泛化误差"><a href="#4-3-2-泛化误差" class="headerlink" title="4.3.2 泛化误差"></a>4.3.2 泛化误差</h3><p>泛化是指机器学习对未知的测试输入样本的处理能力。</p><p>泛化误差是指对未知的测试输入样本的输出所做的预测的误差。</p><p>在泛化误差的评价过程中，尤为重要的是泛化误差不一定要与训练样本的误差相一致。因此，对于训练样本的误差，并不是要选择平方误差 $J_{LS}$ 最小时对应的模型。</p><p>在实际应用中，通常采用 <strong>交叉验证法</strong> 评价泛化误差。即，把训练样本的一部分（通常取 $2-10$ 个）拿出来作为测试样本，不将其用于学习，而只用于评价最终学习结果的泛化误差。</p>]]></content>
    
    
    
    <tags>
      
      <tag>MachineLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【笔记】机器学习第三章 最小二乘学习法</title>
    <link href="/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E7%AB%A0-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%AD%A6%E4%B9%A0%E6%B3%95/"/>
    <url>/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E7%AB%A0-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%AD%A6%E4%B9%A0%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="第三章-最小二乘学习法"><a href="#第三章-最小二乘学习法" class="headerlink" title="第三章 最小二乘学习法"></a>第三章 最小二乘学习法</h1><h2 id="3-1-最小二乘估计-（Least-Squares-Measure）"><a href="#3-1-最小二乘估计-（Least-Squares-Measure）" class="headerlink" title="3.1 最小二乘估计 （Least Squares Measure）"></a>3.1 最小二乘估计 （Least Squares Measure）</h2><p><strong>模型的输出</strong>： $f_\mathbf{\theta} (\mathbf{x}_i)$，即拟合值；</p><p><strong>训练集的输出</strong>：$\mathbf{y} = {y_i}$，即真值；</p><p>所以，两者的残差就是：$f_\mathbf{\theta} (\mathbf{x}_i) - \mathbf{y}$；</p><p>取残差的 $l2$ 范数：</p><script type="math/tex; mode=display">J_{LS}(\mathbf{\theta}) = \frac{1}{2}\left \| f_\mathbf{\theta} (\mathbf{x}_i) - \mathbf{y} \right \|^2 = \frac{1}{2} \sum_{i=1}^n (f_\mathbf{\theta} (\mathbf{x}_i) - y_i)^2</script><p>表示模型输出 $f_\mathbf{\theta} (\mathbf{x}_i)$ 和 训练集输出 $\mathbf{y} = {y_i}$ 的 <strong>平方误差</strong></p><p>用向量形式表示模型的基函数：</p><script type="math/tex; mode=display">\mathbf{\Phi} = \mathbf{\phi}(\mathbf{x}) = \left[ \begin{matrix} \phi_1(\mathbf{x}_1) & \cdots & \phi_b(\mathbf{x}_1) \\  \vdots & \ddots & \vdots \\ \phi_1(\mathbf{x}_n) & \cdots & \phi_b(\mathbf{x}_n) \end{matrix} \right]</script><p>称为 $n \times b$ 阶设计矩阵；</p><p>这样就可以把线性模型的输出表示为：</p><script type="math/tex; mode=display">\mathbf{f}_\mathbf{\theta} (\mathbf{x}_i) = \mathbf{\theta}^\top \mathbf{\phi}(\mathbf{x}) = \mathbf{\Phi} \mathbf{\theta}</script><p>则<strong>平方误差</strong> $J_{LS}(\mathbf{\theta})$ 就变形为：</p><script type="math/tex; mode=display">J_{LS}(\mathbf{\theta}) = \frac{1}{2}\left \| \mathbf{\Phi} \mathbf{\theta} - \mathbf{y} \right \|^2</script><p>我们的希望是：<strong>找到一个 $\theta$，使得在这个值下的平方误差 $J_{LS}(\mathbf{\theta})$ 最小</strong>；</p><p>用数学语言描述就是：<strong>$J_{LS}(\mathbf{\theta})$  对 $\theta$ 的偏导（梯度）为 $0$ 时候，$\theta$ 满足最小二乘解</strong>，即：</p><script type="math/tex; mode=display">\nabla_\theta J_{LS} = \mathbf{\Phi}^\top \mathbf{\Phi} \mathbf{\theta} - \mathbf{\Phi}^\top \mathbf{y} = 0</script><p>变形得到：</p><script type="math/tex; mode=display">\mathbf{\Phi}^\top \mathbf{\Phi} \mathbf{\theta} = \mathbf{\Phi}^\top \mathbf{y}</script><p>这个方程的解 $\mathbf{\theta} = \hat{\mathbf{\theta}}_{LS}$ 就是我们要求的<strong>最小二乘解</strong>：</p><script type="math/tex; mode=display">\hat{\mathbf{\theta}}_{LS} = \mathbf{\Phi}^\dagger \mathbf{y}</script><p>其中，$\mathbf{\Phi}^\dagger$表示广义逆：$\mathbf{\Phi}^\dagger = (\mathbf{\Phi}^\top \mathbf{\Phi})^{-1} \mathbf{\Phi}^\top$。</p><h2 id="3-2-最小二乘解的性质"><a href="#3-2-最小二乘解的性质" class="headerlink" title="3.2 最小二乘解的性质"></a>3.2 最小二乘解的性质</h2><ul><li><strong>奇异值分解</strong> $\mathbf{\Phi} = \sum_{k=1}^{\min(n,b)} \kappa_k \psi_k \varphi_k$</li></ul><p>其中 $\kappa_k$ 称为奇异值，总是非负；$\psi_k$ 和 $\varphi_k$ 为左、右奇异向量，都是满足正交性。</p><ul><li>在MATLAB中可以使用 <code>svd()</code> 函数进行奇异值分解。</li></ul><p><strong>性质一</strong>：通过奇异值分解，$\mathbf{\Phi}^\dagger$ 就可以表示为：</p><script type="math/tex; mode=display">\mathbf{\Phi}^\dagger = \sum_{k=1}^{\min(n,b)} \kappa_k^\dagger \varphi_k \psi_k^\top</script><p>所以，最小二乘解 $\hat{\mathbf{\theta}}_{LS}$ 可以表示为：</p><script type="math/tex; mode=display">\hat{\mathbf{\theta}}_{LS}= \mathbf{\Phi}^\dagger \mathbf{y}= \sum_{k=1}^{\min(n,b)} \kappa_k^\dagger (\psi_k^\top \mathbf{y}) \varphi_k</script><p>于是，最小二乘模型的输出就表示为：</p><script type="math/tex; mode=display">\mathbf{f}_\mathbf{\theta} (\mathbf{x}_i) = \mathbf{\Phi} \mathbf{\theta} = \mathbf{\Phi} \hat{\mathbf{\theta}}_{LS} = \mathbf{\Phi} \mathbf{\Phi}^\dagger \mathbf{y}</script><p>上式中的 $\mathbf{\Phi} \mathbf{\Phi}^\dagger$ 是 $\mathbf{\Phi}$ 的值域 $\Re(\mathbf{\Phi})$ 的正投影矩阵，因此，模型的输出 $\mathbf{f}_{\theta} (\mathbf{x}_i)$ 可以看作是由设计矩阵 $\mathbf{\Phi}$ 的正投影得到的。</p><p><strong>性质二</strong>：最小二乘解 $\hat{\mathbf{\theta}}_{LS}$ 是真实的 $\mathbf{\theta}^\star$ 的无偏估计量。</p><p><strong>性质三</strong>：渐进无偏性。随者训练样本数 $n$ 的增加，最小二乘解 $\hat{\mathbf{\theta}}_{LS}$ 和其真实值 $\mathbf{\theta}^\star$ 的误差会逐渐向模型中的最优参数方向收敛。</p><h2 id="3-3-大规模数据的学习算法（随机梯度法）"><a href="#3-3-大规模数据的学习算法（随机梯度法）" class="headerlink" title="3.3 大规模数据的学习算法（随机梯度法）"></a>3.3 大规模数据的学习算法（随机梯度法）</h2><p><strong>为什么会有随机梯度法</strong> 设计矩阵 $\mathbf{\Phi}$ 的维度为 $n \times b$ ，当训练样本数 $n$ 或参数个数 $b$ 很大时，设计矩阵的运算会十分复杂导致计算机内存不足；这时使用随机梯度法会很好的降低计算迭代次数和计算机内存消耗。</p><p><strong>什么是随机梯度法</strong> 沿着训练平方误差 $J_{LS}$ 的梯度下降，对参数 $\mathbf{\theta}$ 依次进行学习的算法：</p><p>一般情况下，训练平方误差 $J_{LS}$ 是凸函数，所以它的局部最优解就是全局最优解；而随机梯度法就是寻找这个局部最优解。</p><p><strong>随机梯度法的算法流程</strong> </p><ol><li><p>选定适当的 $\mathbf{\theta}$ 初值。</p></li><li><p>随机选择一个样本 $(\mathbf{x}_k, y_k)$。</p></li><li><p>对于选定的样本，采用使其梯度下降的方式，对参数 $\mathbf{\theta}$ 进行更新。</p></li><li><p>重复 2.、3. 步直到 $\mathbf{\theta}$ 达到收敛精度。</p></li></ol><p>具体使用下式计算每一个 $\mathbf{\theta}$：</p><script type="math/tex; mode=display">\mathbf{\theta} = \mathbf{\theta} - \varepsilon \nabla_\theta J_{LS}^{(k)}(\mathbf{\theta})</script><p>式中：</p><script type="math/tex; mode=display">\nabla_\theta J_{LS}^{(k)}(\mathbf{\theta}) = \phi(\mathbf{x}_i)(f_{\mathbf{\theta}}(\mathbf{x}_i) - y_i)</script><p>是训练平方误差的梯度，表示梯度下降的方向；$\varepsilon$ 是学习系数，表示梯度下降的步幅。</p><p><strong>Needs to know</strong> 随机梯度法的收敛速度强烈依赖于梯度下降步幅 $\varepsilon$，因此，选取合适的 $\varepsilon$ 值十分重要。然而实际中选择最优的 $\varepsilon$ （调参）也是很困难的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>MachineLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【笔记】机器学习第二章 学习模型</title>
    <link href="/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
    <url>/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="第二章-学习模型"><a href="#第二章-学习模型" class="headerlink" title="第二章 学习模型"></a>第二章 学习模型</h1><h2 id="2-1-线性模型"><a href="#2-1-线性模型" class="headerlink" title="2.1 线性模型"></a>2.1 线性模型</h2><p><strong>最简单的线性模型</strong> 输入是一维时，学习对象函数 $f = \theta \times x$，$\theta$ 表示模型的参数。这个模型只能表示线性的输入输出，在实际应用中没有很大价值。</p><p><strong>基于参数的线性模型</strong> 对最简单的线性模型进行扩展：</p><p>将一维的输入 $x$ 扩展为 $b$ 个基函数向量 $\mathbf{\phi} (x) = (\phi_1(x), \phi_2(x), …, \phi_b (x))^\top$；</p><p>将参数 $\theta$ 扩展为 $b$ 维参数向量 $\mathbf{\theta} = (\theta_1, \theta_2, …, \theta_b)^\top$；</p><p>就可以将简单的线性模型扩展为：</p><script type="math/tex; mode=display">\mathbf{f_\theta}(x) = \sum_{j=1}^b \theta_j \phi_j(x) = \mathbf{\theta}^\top \mathbf{\phi}(x)</script><p>这时，该模型任然是基于参数向量 $\mathbf{\theta}$ 的线性形式。</p><p>当基函数 $\mathbf{\phi}(x)$ 变为多项式形式 $\mathbf{\phi}(x) = (1, x, x^2, …, x^{b-1})^\top$ 或 $b = 2m +1$ 的三角多项式形式 $\mathbf{\phi}(x) = (1, \sin x, \cos x, \sin 2x, \cos 2x, …, \sin mx, \cos mx)^\top$ 时，该线性模型就可以表示复杂的非线性模型了。</p><p><strong>基于参数的多维线性模型</strong> 在<strong>基于参数的线性模型</strong>中， 如果将输入 $x$ 继续扩展为 $d$ 维向量形式 $\mathbf{x} = (x^{(1)}, x^{(2)}, …, x^{(d)})^\top$ 则可以得到更复杂的多维模型：</p><script type="math/tex; mode=display">\mathbf{f_\theta}(\mathbf{x}) = \sum_{j=1}^b \theta_j \phi_j(\mathbf{x}) = \mathbf{\theta}^\top \mathbf{\phi}(\mathbf{x})</script><ul><li><strong>加法模型</strong> 把一维基函数作为因子，通过多个一维基函数相加而获得多维基函数的方法；它的所有参数个数是 $b’d$ 只会随者输入维数 $d$ 呈线性增长；所需算力更小，但是它的表现力更差。</li></ul><script type="math/tex; mode=display">\mathbf{f_\theta}(\mathbf{x}) = \sum_{k=1}^d \sum_{j=1}^{b'} \theta_{k, j} \phi_j(\mathbf{x^{(k)}})</script><ul><li><strong>乘法模型</strong> 把一维基函数作为因子，通过多个一维基函数相乘而获得多维基函数的方法；它的所有参数个数是 $(b’)^d$， 会随者输入维数 $d$ 呈指数增长；它的表现力更强，但是所需算力更大。</li></ul><script type="math/tex; mode=display">\mathbf{f_\theta}(\mathbf{x}) = \sum_{j1=1}^{b'} ... \sum_{jd=1}^{b'} \theta_{j1, ... , jd} \phi_{j1}(x^{(1)}) ... \phi_{jd}(x^{(d)})</script><p>以上三种线性模型的基函数与训练样本 ${(\mathbf{x}_i, y_i)}$ 均是无关的；</p><p>即，<strong>基函数的选取完全依靠经验和猜测，训练样本并不提供基函数选择的参考</strong>。</p><h2 id="2-2-核模型"><a href="#2-2-核模型" class="headerlink" title="2.2 核模型"></a>2.2 核模型</h2><p><strong>核模型</strong> 以二元核函数 $K(\mathbf{x},\mathbf{x}_j)$ 的线性组合的方式进行定义：</p><script type="math/tex; mode=display">\mathbf{f_\theta}(\mathbf{x}) = \sum_{j=1}^n \theta_j K(\mathbf{x},\mathbf{x}_j)</script><p><strong>核模型的特点</strong> </p><ul><li><p>不同于线性模型，核模型的构建需要根据输入变量 $\mathbf{x}$ 进行决定；但是它不依赖于 $\mathbf{x}$ 的维数 $d$，而只由训练样本数 $n$ 决定。因此，即便输入维数 $d$ 很大，只要训练样本数 $n$ 不大，也在普通计算机的承受范围内；而即使训练样本数 $n$ 很大，只要输入样本 ${\mathbf{x}_i}$ 的部分集合 ${\mathbf{c}_j}$ 作为核均值来进行计算，计算负荷也可以很好的抑制。</p></li><li><p>核模型也是参数向量 $\mathbf{\theta} = (\theta_1, \theta_2, …, \theta_b)^\top$ 的线性形式，因此可以看作是基于参数的线性模型的特例。</p></li><li><p><strong>核映射方法</strong> 当输入样本 $x$ 不是向量的时候，核模型也可以很容易的实现扩展。在核模型中，输入样本 $x$ 只存在于核函数 $K(\mathbf{x},\mathbf{x}_j)$ 中，因此，只需对两个输入样本 $x$ 和 $x_j$ 相对应的核函数加以定义即可，而不用关心输入样本 $x$ 具体是什么。</p></li></ul><p><strong>高斯核函数</strong> 在众多核函数中，使用最为广泛：</p><script type="math/tex; mode=display">K(\mathbf{x}, \mathbf{c}) = \exp(-\frac{\left \| \mathbf{x} - \mathbf{c} \right \|^2}{2h^2})</script><p>其中，$\left | \mathbf{x} - \mathbf{c} \right |^2$ 表示 $\mathbf{x} - \mathbf{c}$ 的 $l^2$ 范数，即，$\left | \mathbf{x} - \mathbf{c} \right |^2 = \sqrt{(\mathbf{x} - \mathbf{c})^\top (\mathbf{x} - \mathbf{c})}$；$h$ 和 $\mathbf{c}$ 分别对应于高斯核函数的带宽与均值。</p><p>这样，核模型就变成了高斯核模型：</p><script type="math/tex; mode=display">\mathbf{f_\theta}(\mathbf{x}) = \sum_{i=1}^n \theta_i K(\mathbf{x},\mathbf{x}_i) = \sum_{i=1}^n \theta_i \exp(-\frac{\left \| \mathbf{x} - \mathbf{\mathbf{x}_i} \right \|^2}{2h^2})</script><p>对每个输入样本 ${\mathbf{x}_i}$ 进行高斯核配置，并把其高度 ${\mathbf{\theta}_i}$ 作为参数进行学习。</p><p>因此，在高斯核模型中，一般只能在训练集的输入样本附近对函数进行近似，这也减轻了维数灾难。</p><h2 id="2-3-层级模型"><a href="#2-3-层级模型" class="headerlink" title="2.3 层级模型"></a>2.3 层级模型</h2><p>以上的<strong>线性模型</strong>和<strong>核模型</strong>都是关于参数向量的线性函数，它们都是线性模型；如果学习模型是关于参数的非线性关系，那它就是<strong>非线性模型</strong>。其中最常用的是<strong>层级模型</strong>：</p><script type="math/tex; mode=display">\mathbf{f_\theta}(\mathbf{x}) = \sum_{j=1}^b \alpha_j \phi(\mathbf{x};\beta_j)</script><p>其中，$\phi(\mathbf{x};\beta_j)$ 是基函数，其中含有参数向量 $\mathbf{\beta}$；$\mathbf{\alpha}$ 也是参数向量，所以层级模型可以被看作是基于 $\mathbf{\alpha}$ 的线性模型；同时，基函数中又含有参数向量 $\mathbf{\beta}$，所以也可以看作是基于 $\mathbf{\phi} = (\mathbf{\alpha}^\top, \mathbf{\beta}_1^\top, …, \mathbf{\beta}_b^\top)^\top$ 的非线性模型。</p><p>同样，层级模型中的基函数选取也很多样，通常采用 S 函数或者高斯函数。</p><ul><li><strong>S 型函数</strong>模仿的是人类脑细胞的输入输出，也被称为人工神经网络模型。</li></ul><script type="math/tex; mode=display">\phi(\mathbf{x};\mathbf{\beta})_{S-func} = \frac{1}{1 + \exp (-\mathbf{x}^\top \mathbf{\omega} - \gamma)}, \qquad \mathbf{\beta} = (\mathbf{\omega}^\top, \gamma)^\top</script><ul><li><strong>高斯函数</strong>与核模型中使用的高斯函数相同，但是这里的均值和带宽都是进行学习得到的。因此，层级模型一般比核模型能过更加灵活地对函数进行近似。</li></ul><script type="math/tex; mode=display">\phi(\mathbf{x};\mathbf{\beta})_{Gaussian-func} = \exp (- \frac{\left \| \mathbf{x} - \mathbf{c} \right \|^2}{2h^2}), \qquad \mathbf{\beta} = (\mathbf{c}^\top, h)^\top</script><p>层级模型地学习过程较艰难，常采用贝叶斯学习的方法或者从邻近输入样本的层级开始逐层进行无监督学习来初始化模型。</p>]]></content>
    
    
    
    <tags>
      
      <tag>MachineLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【笔记】机器学习第一章 什么是机器学习</title>
    <link href="/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0-%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0-%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="第一章-什么是机器学习"><a href="#第一章-什么是机器学习" class="headerlink" title="第一章 什么是机器学习"></a>第一章 什么是机器学习</h1><h2 id="1-1-学习的种类"><a href="#1-1-学习的种类" class="headerlink" title="1.1 学习的种类"></a>1.1 学习的种类</h2><p><strong>监督学习</strong> 根据学习过程中所获得的经验、技能，对没有学习过的问题也能做出正确的解答。eg. 回归、分类、排序。</p><p><strong>无监督学习</strong> 计算机在互联网中自动收集信息，并从中获取有用的信息；它不局限在解决有明确答案的问题，学习目标可以不很明确。eg. 异常检测、聚类。</p><p><strong>强化学习</strong> 在没有老师提示对错的情况下，自己对预测的结果进行评估的方法。eg. 分类、聚类、降维。</p><h2 id="1-2-机器学习任务的例子"><a href="#1-2-机器学习任务的例子" class="headerlink" title="1.2 机器学习任务的例子"></a>1.2 机器学习任务的例子</h2><h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a><strong>回归</strong></h3><p>把实函数在样本附近加以近似的有监督的函数近似问题；对一个或多个自变量和因变量之间的关系进行建模、求解的一种统计方法。</p><ul><li>以 $d$ 维实向量 $\mathbf{x}$ 作为输入，实数值 $y$ 作为输出的函数 $y = f(\mathbf{x})$ 的学习问题为例：</li></ul><p>在监督学习里，真实的函数关系 $f$ 是未知的，训练集输入输出样本 $\lbrace (\mathbf{x}_i, y_i) \rbrace$ 是已知的；同时输出样本 $y_i$ 的真实值 $f(\mathbf{x} _i)$ 中会观察到噪声。</p><p>学习过程就是：将输入样本 $\mathbf{x}_i$ 解答为输出样本 $y_i$，中间的解答过程用 $\hat{f}$ 表示。获得 $\hat{f}$ 函数就是监督学习的最终目标，而 $\hat{f}$ 与 $f$ 的相似性可以用来分析 $\hat{f}$ 的泛化能力。</p><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a><strong>分类</strong></h3><p>对于指定的模式进行识别的有监督的模式识别问题；也可以被看作是函数近似的问题（回归也是）。</p><ul><li>以 $d$ 维实向量 $\mathbf{x}$ 作为输入，且所有输入样本可以被划分为 $c$ 个类别的学习问题为例：</li></ul><p>输入输出样本 ${(\mathbf{x}_i, y_i)}$ 是已知的；但是输出样本 $y_i$ 并不是具体的实数，而是分别代表类别 $1, 2, …, c$。</p><p>学习过程就是：从输入样本 $\mathbf{x}_i$ 得到输出类别 $1, 2, …, c$ 的函数 $y = f(\mathbf{x})$ 的过程。</p><p>与回归不同，分类只是单纯对样本应该属于哪一类别进行预测，而不存在“类别A比类别C跟接近类别B”这样的说法。</p><h3 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a><strong>异常检测</strong></h3><p>寻找输入样本 ${\mathbf{x}_i}$ 中所包含的异常数据的问题。</p><p>在已知正常数据和异常数据的例子的情况下，与有监督的分类问题相同；在异常数据未知的情况下，就成为无监督学习问题，这时常采用密度估计的方法（把靠近密度中心的数据作为正常数据，远离的作为异常数据）。</p><h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a><strong>聚类</strong></h3><p>只给出输入样本 ${\mathbf{x}_i}$，然后判断各个样本分别属于 $1, 2, …, c$ 中的哪个簇。是无监督学习的模式识别问题</p><h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a><strong>降维</strong></h3><p>从高纬度数据中提取关键信息，将其转换为易于计算的低纬度问题而求解的方法；将高维的输入样本 ${\mathbf{x}_i}$ 转换为低维度的样本 ${\mathbf{z}_i}$ 的的方法。</p><p>线性降维的情况下，存在横向量 $\mathbf{T}$，使得 $\mathbf{z}_i = \mathbf{T}\mathbf{x}_i$。</p><p>当输入输出样本 ${(\mathbf{x}_i, y_i)}$ 是已知时，属于监督学习，可以把样本转换为较低维度的样本 ${\mathbf{z}_i}$，从而获得较高的泛化能力。</p><p>当只有输入样本 ${\mathbf{x}_i}$ 是已知时，属于无监督学习，样本转换为较低维度的样本 ${\mathbf{z}_i}$ 后，应该保持原始输入样本 ${\mathbf{x}_i}$ 的数据分布性质以及数据间的近邻关系不变。</p><h2 id="1-3-机器学习的方法"><a href="#1-3-机器学习的方法" class="headerlink" title="1.3 机器学习的方法"></a>1.3 机器学习的方法</h2><ul><li>后验概率 $p(y\mid \mathbf{x})$ 与生成概率 $p(\mathbf{x}, y)$：</li></ul><script type="math/tex; mode=display">p(y\mid \mathbf{x}) = \frac{p(\mathbf{x}, y)}{p(\mathbf{x})} = \frac{p(\mathbf{x}, y)}{\sum_{y} p(\mathbf{x},y)} \propto p(\mathbf{x}, y)</script><hr><ul><li>以对模式 $\mathbf{x}$ 的类别 $y$ 进行预测的分类问题为例。</li></ul><h3 id="生成式分类"><a href="#生成式分类" class="headerlink" title="生成式分类"></a><strong>生成式分类</strong></h3><p>通过预测数据的生成概率 $p(\mathbf{x},y)$ 来进行模式识别的分类方法。</p><h3 id="判别式分类"><a href="#判别式分类" class="headerlink" title="判别式分类"></a><strong>判别式分类</strong></h3><p>应用训练集直接对后验概率 $p(y\mid \mathbf{x})$ 进行学习的过程。</p><p>有时，我们手中的数据不足以解决一般性问题，但是足够解决特定问题；后验概率（特定问题的解）可以由生成概率推出，但是生成概率（一般性问题的解）不能由后验概率得到，则判别式分类就是比生成式分类更好的机器学习方法。</p><p>另一方面，在其他的一些问题中，我们可以获得一些数据生成概率的先验知识，这时，生成式分类就是更好的方法。</p><hr><ul><li>以包含参数 $\mathbf{\theta}$ 的模型 $q(\mathbf{x}, y; \mathbf{\theta})$ 为例。</li></ul><h3 id="频率派"><a href="#频率派" class="headerlink" title="频率派"></a><strong>频率派</strong></h3><p>事物是服从一个参数未知、但固定的分布。</p><p>将模式 $\mathbf{\theta}$ 作为决定论的变量，使用手头的训练样本 $\xi = {(\mathbf{x}_i, y_i)}$ 对模式 $\mathbf{\theta}$ 进行学习。如何由训练集 $\xi$ 得到高精度的模式 $\mathbf{\theta}$ 是主要的研究课题。</p><p>在最大似然算法中，一般对生成训练集 $\xi$ 的最容易的方法所对应的模式 $\xi$ 进行学习：</p><script type="math/tex; mode=display">\max \limits_ \theta \prod_{i=1}^y q(\mathbf{x}_i, y_i; \mathbf{\theta})</script><h3 id="贝叶斯派"><a href="#贝叶斯派" class="headerlink" title="贝叶斯派"></a><strong>贝叶斯派</strong></h3><p>事物是不确定的，人们只能找到最优（但是不可能准确）的概率分布。</p><p>将模式 $\mathbf{\theta}$ 作为概率变量，对其先验概率 $p(\mathbf{\theta})$ 加以考虑，计算与训练集 $\xi$ 相对应的后验概率 $p(\mathbf{\theta} \mid \xi)$。如何精确计算后验概率是主要的研究课题。</p><p>在贝叶斯估计中，可以使用先验概率 $p(\mathbf{\theta})$ 来求解后验概率 $p(\mathbf{\theta} \mid \xi)$：</p><script type="math/tex; mode=display">p(\mathbf{\theta} \mid \xi) = \frac{p(\xi \mid \mathbf{\theta})p(\mathbf{\theta})}{p(\xi)} = \frac{\prod_{i=1}^n q(\mathbf{x}_i, y_i \mid \mathbf{\theta})p(\mathbf{\theta})}{\int\prod_{i=1}^n q(\mathbf{x}_i, y_i \mid \mathbf{\theta})p(\mathbf{\theta}) d\mathbf{\theta}}</script><hr><h3 id="本书主要讲解基于频率派的判别派机器学习算法。"><a href="#本书主要讲解基于频率派的判别派机器学习算法。" class="headerlink" title="本书主要讲解基于频率派的判别派机器学习算法。"></a>本书主要讲解基于<strong>频率派</strong>的<strong>判别派</strong>机器学习算法。</h3>]]></content>
    
    
    
    <tags>
      
      <tag>MachineLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【笔记】图像质量量化评估综述</title>
    <link href="/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E9%87%8F%E5%8C%96%E8%AF%84%E4%BC%B0%E7%BB%BC%E8%BF%B0/"/>
    <url>/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E9%87%8F%E5%8C%96%E8%AF%84%E4%BC%B0%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="文章框架："><a href="#文章框架：" class="headerlink" title="文章框架："></a>文章框架：</h1><p><img src="/image/imgEvalu.png" srcset="/img/loading.gif" alt=""></p><h1 id="1-为什么需要图像质量评估标准（QA）"><a href="#1-为什么需要图像质量评估标准（QA）" class="headerlink" title="1. 为什么需要图像质量评估标准（QA）"></a>1. 为什么需要图像质量评估标准（QA）</h1><p>QA (Quality Assessment) 用以评价图片/视频在变换之后质量损失的程度。</p><h1 id="2-评估标准分类"><a href="#2-评估标准分类" class="headerlink" title="2. 评估标准分类"></a>2. 评估标准分类</h1><h2 id="2-1-主观评价"><a href="#2-1-主观评价" class="headerlink" title="2.1 主观评价"></a>2.1 主观评价</h2><p>观看者对失真图像进行评分，求得平均主管得分（Mean Opinion Score）。</p><h2 id="2-2-客观评价"><a href="#2-2-客观评价" class="headerlink" title="2.2 客观评价"></a>2.2 客观评价</h2><p>使用某种数学模型给出参考图像和评估图像之间的差异量化值。</p><h3 id="2-2-1-全参考-（Full-Reference-FR-IQA）"><a href="#2-2-1-全参考-（Full-Reference-FR-IQA）" class="headerlink" title="2.2.1 全参考 （Full Reference, FR-IQA）"></a>2.2.1 全参考 （Full Reference, FR-IQA）</h3><p>对比无失真的原始图像和评价图像的信息量或特征相似度；最为充分和成熟；本文主要讨论。</p><h3 id="2-2-2-半参考-（Reduced-Reference-RR-IQA）"><a href="#2-2-2-半参考-（Reduced-Reference-RR-IQA）" class="headerlink" title="2.2.2 半参考 （Reduced Reference, RR-IQA）"></a>2.2.2 半参考 （Reduced Reference, RR-IQA）</h3><p>介于上下两者之间，从参考图像中提取部分特征。</p><h3 id="2-2-3-无参考-（No-Reference-NR-IQA）"><a href="#2-2-3-无参考-（No-Reference-NR-IQA）" class="headerlink" title="2.2.3 无参考 （No Reference, NR-IQA）"></a>2.2.3 无参考 （No Reference, NR-IQA）</h3><p>完全没有参考图像或者参考图像没有有用特征；挑战高。</p><h1 id="3-评估方式"><a href="#3-评估方式" class="headerlink" title="3.评估方式"></a>3.评估方式</h1><h2 id="3-1-一般图片评估方式"><a href="#3-1-一般图片评估方式" class="headerlink" title="3.1 一般图片评估方式"></a>3.1 一般图片评估方式</h2><h3 id="3-1-1-PSNR"><a href="#3-1-1-PSNR" class="headerlink" title="3.1.1. PSNR"></a>3.1.1. PSNR</h3><p>峰值信噪比 PSNR (Peak Signal to Noise Ratio) 基于均方误差计算图像失真程度。</p><p><img src="/image/PSNR.png" srcset="/img/loading.gif" alt=""></p><p>最为广泛，但是不够合理；也不接近人眼的直观感觉；没有考虑到人的视觉中亮度敏感度强于色度。</p><h3 id="3-1-2-SSIM"><a href="#3-1-2-SSIM" class="headerlink" title="3.1.2. SSIM"></a>3.1.2. SSIM</h3><p>结构相似度 SSIM (Structural Similarity Index) 使用可见性误差函数 (Visibility of Error Function) 和人眼观看习惯（整体先于细节、纹理变化剧烈先于变化缓慢）加权计算。</p><p>由亮度对比、对比度对比、结构对比三部分组成：</p><p><img src="/image/SSIM.png" srcset="/img/loading.gif" alt=""></p><p>类比人眼观察，较 PSNR 更加合理。</p><p>ref. <a href="https://www.cns.nyu.edu/pub/lcv/wang03-preprint.pdf" target="_blank" rel="noopener">https://www.cns.nyu.edu/pub/lcv/wang03-preprint.pdf</a></p><h3 id="3-1-3-MS-SSIM"><a href="#3-1-3-MS-SSIM" class="headerlink" title="3.1.3. MS-SSIM"></a>3.1.3. MS-SSIM</h3><p>多尺度结构相似度 MS-SSIM (Multi Scale Structural Similarity Index) 在不同分辨率 (尺度) 下多次计算 SSIM 后综合得到最终的评价值。</p><p><img src="/image/MS-SSIM.png" srcset="/img/loading.gif" alt=""></p><p>图像到观看者的距离、像素信息密集程度等因素均会对观看者给出的主观评价产生影响。</p><p>ref. <a href="https://ece.uwaterloo.ca/~z70wang/publications/msssim.pdf" target="_blank" rel="noopener">https://ece.uwaterloo.ca/~z70wang/publications/msssim.pdf</a></p><h3 id="3-1-4-IW-SSIM"><a href="#3-1-4-IW-SSIM" class="headerlink" title="3.1.4. IW-SSIM"></a>3.1.4. IW-SSIM</h3><p>基于信息量加权的结构相似度方案 IW-SSIM（Evaluation of Information Content-Weighted SSIM）引入空间变化的权重，计算更加符合人眼观察实际的客观评价值。</p><h2 id="3-2-全景图片评估方式"><a href="#3-2-全景图片评估方式" class="headerlink" title="3.2 全景图片评估方式"></a>3.2 全景图片评估方式</h2><h3 id="3-2-1-360Lib"><a href="#3-2-1-360Lib" class="headerlink" title="3.2.1. 360Lib"></a>3.2.1. 360Lib</h3><p>360Lib 是 JVET 小组设计并面向使用者公开的全景视频研究平台，内置了诸多图像/视频质量评价标准。</p><p>各种投影方案的优缺点：</p><p><img src="/image/360Lib.png" srcset="/img/loading.gif" alt=""></p><p>ref. <a href="http://jvet.hhi.fraunhofer.de/" target="_blank" rel="noopener">http://jvet.hhi.fraunhofer.de/</a></p><h3 id="3-2-2-WS-PSNR"><a href="#3-2-2-WS-PSNR" class="headerlink" title="3.2.2. WS-PSNR"></a>3.2.2. WS-PSNR</h3><p>WS-PSNR（Weighted to Spherically uniform PSNR）通过引入权重的方式对源视频与输出视频直接计算 PSNR。</p><h3 id="3-2-3-S-PSNR"><a href="#3-2-3-S-PSNR" class="headerlink" title="3.2.3. S-PSNR"></a>3.2.3. S-PSNR</h3><p>S-PSNR（Spherical PSNR）使用球面上一系列预先确定位置的点来对参考图像和测试图像进行采样，比较它们两两之间的差值，累加得到整幅图像的S-PSNR值。</p><h3 id="3-2-4-CPP-PSNR"><a href="#3-2-4-CPP-PSNR" class="headerlink" title="3.2.4. CPP-PSNR"></a>3.2.4. CPP-PSNR</h3><p>克拉斯特抛物线投影（Craster’s Parabolic Projection，CPP）是一种在地图学上常用的伪圆柱等面积投影方式，CPP投影在相同空间分辨率下形状失真优于ERP投影；计算过程中，将输入序列从ERP格式转换成CPP格式，再由CPP变换到其他的投影格式进行编码和后续操作。</p>]]></content>
    
    
    
    <tags>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【读书笔记】乌合之众：大众心理研究</title>
    <link href="/%E3%80%90%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E3%80%91%E4%B9%8C%E5%90%88%E4%B9%8B%E4%BC%97%EF%BC%9A%E5%A4%A7%E4%BC%97%E5%BF%83%E7%90%86%E7%A0%94%E7%A9%B6/"/>
    <url>/%E3%80%90%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E3%80%91%E4%B9%8C%E5%90%88%E4%B9%8B%E4%BC%97%EF%BC%9A%E5%A4%A7%E4%BC%97%E5%BF%83%E7%90%86%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="导读：群体的时代"><a href="#导读：群体的时代" class="headerlink" title="导读：群体的时代"></a>导读：群体的时代</h2><p>1. 时代演变是民族精神的深刻改变</p><p>2.当代（法国大革命时期），人类的精神正在经历质变：旧（政治、宗教、社会信仰）未灭，新（科学、工业、思想）未至</p><p>3. 群体对理性并不适应，但总是急于行动</p><p>4. 任何领导人都有意或无意的深谙群体心理学</p><p>5. 群体无法接受任何观点，只能“骗”。领导群体的基本原则是：不能把他们平等的当人看，只能求助于能打动和诱惑他们的手段</p><a id="more"></a><h2 id="第一卷：群体的性格"><a href="#第一卷：群体的性格" class="headerlink" title="第一卷：群体的性格"></a>第一卷：群体的性格</h2><h3 id="第一章：基本特征：性格统一律"><a href="#第一章：基本特征：性格统一律" class="headerlink" title="第一章：基本特征：性格统一律"></a>第一章：基本特征：性格统一律</h3><p><strong>1. 心理学意义上的“群体”：</strong> 在某些条件下，且只在这些条件下，人群会表现出某些全新的特点，群体成员的思想感情有一种互相统一的倾向，自觉的个性消失，出现暂时性、统一和鲜明的性格。</p><p><strong>2. 如同每个人心理不会一成不变一样，群体心理会因民族和组成成分不同而各异：</strong> 也会因群体正受何种性质和强度的刺激因素支配而不同。</p><p><strong>3. 群体特征1：</strong> 个体融入群体后，都会被群体的性格改变，不是单纯的所有个体平均，而是产生了新的思想感情和行为方式。这种变换的程度容易证明，但是原因未知。</p><p><strong>4. 群体特征2：</strong> 无意识不仅支配生理现象，还主宰意识的运行（现代心理学）。我们的意识活动只是精神底层的无意识运行的结果，这个精神底层就是代代相传的名族性格，也就是人们的种种共性。aka. 无意识构成了民族性格，所以一个国家的人都很像，只因为个性中的意识层面不同而不同，这可能由教育、先天因素、生活环境导致的结果。</p><p><strong>5. 群体特征3：</strong> 在群体中，个人理智降低，个性被削弱。同一性吞没了特异性，无意识属性主导群体行为。因而，群体的智慧并不叠加，叠加出来的都是愚蠢。</p><p><strong>6. 群体特征出现的原因1：</strong> 个体聚集后，会因为“变成复数”而敢于发泄本来压抑的欲望，获得群体提供的强大感、匿名性和无责任性。</p><p><strong>7. 群体特征出现的原因2：</strong> 群体提供<strong>传染性</strong>。群体中，任何情绪和行为都会传染，而且传染性极大，以至于个体会为了群体利益而放弃个人利益。这种倾向和人的天性完全相反，个体很难做到，除非他成了群体的一员。</p><p><strong>8. 群体特征出现的原因3：</strong> 群体提供易受暗示性。上述两种原因均是易受暗示性的所导致的结果。</p><p><strong>9. 个体构成群体后，个体不再有意识行为</strong> ，他不再是他自己，而变成了不受自己意志控制的机器，一经蛊惑，他就会有遏制不住的冲动去做某事。只有某些<strong>极少数拥有强大的人格的个体可以对抗洪流</strong>，但是群体总是无法接纳这样的个体，群体也会因为这样的个体而分一下神，从而阻止某些惨剧。</p><p><strong>10.</strong> 个体进入群体后，思想感情也会发生彻底的转变，<strong>失去独立性</strong>，允许自己被引诱去做更英勇和狂热的事情，从而<strong>更像野兽</strong>。</p><p><strong>11. 群体比个体更容易犯罪，但也更勇敢；比个体更容易被鼓动，也更容易偏激</strong>；可以说，<strong>群体更加偏向动物性的极端情绪而失去理智</strong>。同时正式这种极端造就了人类历史，假如只计算人类在冷静中做出的伟大行为，世界史上就记不下几件事了。</p><h3 id="第二章：群体的情绪和道德"><a href="#第二章：群体的情绪和道德" class="headerlink" title="第二章：群体的情绪和道德"></a>第二章：群体的情绪和道德</h3><p><strong>1. 群体的冲动性、动态性、狂暴性：</strong> 群体几乎完全只被无意识力量控制，完全不懂思考，是外来刺激的奴隶。群体收到外来刺激时总是做出十分强烈的冲动，同时又总是遵循冲动，所以群体总是处于动态的极端中。<strong>任何信仰的胜利都需要血流成河，群体则为他们提供血液。</strong>群体易变，所以极难管理，因为它的渴望虽然狂热，但是不持久，喜怒无常；同时<strong>它还不允许欲望和欲望的实现之间存在任何障碍。</strong></p><p><strong>2. 民族性格是群体一切情感的来源：</strong> 它持续影响群体的冲动性、动态性和狂暴性，以及下面所述的其他情绪特点。</p><p><strong>3. 轻信性与易感性：</strong> 群体永远徘徊在无意识的领域，没有判断力，和被催眠的人一样：任何进入大脑的信息都有立刻变成行动的倾向，所以群体除了极端轻信之外别无选择。群体的传染性和它只使用形象思维，使得群体很容易创造和扩散神话：传染性使幻觉趋同；形象思维抛弃理性，只是用感性而偏离真相。</p><p><strong>4. 绝大多数人证明的同一件事通常是群体幻觉（与逻辑学相悖）：</strong> 深入人心的是英雄的神话，从来不是真实的英雄；历史书也只是一想象力描述观察有误的事实。</p><p><strong>5. 感情的单向极化：</strong> 群体总是表现出简单化、极端化的情绪，这导致群体从不怀疑，怀疑什么就认为什么是真的，没有不确定的事情。群体习惯了夸张的情绪，所以只能对极端的感情又反应：要打动群体，就要断言、夸张、重复，而不是讲理去证明。</p><p><strong>6. 群体的责任感彻底消失：</strong> 对免责的肯定以及因为人多而一时形成的强大感，使群体成员的情绪和行为偏向原始的狂暴和嗜血，所以群体的暴力倾向大大增强。而这种极化倾向总是被恶意引导，从而走向最糟糕的极端。</p><p><strong>7. 群体排异、专制、保守：</strong> 由于群体极端简单，只懂得极端感情，所以群体总是全盘接受或者全盘否定任何思想观念或信念（催眠出来的信念总是如此，理性产生的信念则不是这样）。群体质疑一切是与非，而且清楚自己的力量，所以既不容忍异见，也喜欢强制服从；在某些群体中，排异性和专制性高度发展，甚至可以摧毁个人独立性，它们认为必须镇压所有异议者，所有人必须无条件服从自己的信念。群体只是表面喜怒无常，实则迷恋和绝对尊重一切传统，恐惧深植于它的无意识中，害怕任何可能改变基本生活状态的新事物。</p><p><strong>8. 群体的道德：</strong> 群体的道德总是走向极端，但总是被恶意利用；群体行为很少考虑个体的利益。但是正是这种被无意识左右、缺乏理性且牺牲个人利益的群体产生了文明和人类历史。</p><h3 id="第三章：群体如何相信、推理和想象"><a href="#第三章：群体如何相信、推理和想象" class="headerlink" title="第三章：群体如何相信、推理和想象"></a>第三章：群体如何相信、推理和想象</h3><p><strong>1. 思想分类1：</strong> 时事引起的偶然思想：它们倏忽即逝。</p><p><strong>2. 思想分类2：</strong> 根本思想：稳定性强，因为环境、基因和公意赋予其力量。</p><p><strong>3. 群体如何获得思想：</strong> 向群体灌输思想，形式必须很绝对、很强硬、很直白；必须化成具象，尽管形象和思想之间没有任何逻辑关系，但是群体很容易接受它们，包括完全相反的思想。</p><p><strong>4. 完全相反的思想：</strong> 群体储存了各种各样的思想，并因地制宜地进行解读，尽管这些思想可能完全相反，但是由于群体完全没有判断力，所以看不到这些矛盾。</p><p><strong>5. 群体只能接受极简形式的思想</strong>，所以一种思想必须进行各种牵强附会、低俗化、直白化后，降低到迎合群体的低智水平才可能在人群中流行；因此，不管一种思想被创造时多么正确伟大，一旦进入群体的理解范围并产生影响力，其伟大和高明就丧失殆尽。</p><p><strong>6. 群体推理的特征：</strong> 把表面有点儿相似的不同东西联系起来，并瞬间把具体经验普遍化。也就是说，群体没有正确的推理能力，由此，群体也没有判断力，它接受直接告诉它的任何判断。而大多数个体也没有高过这个水平。</p><p><strong>7. 群体的想象：</strong> 群体没有推理能力、只具有很强的形象思维能力、联想很跳跃、很容易被彻底影响，导致其没有理性，会放纵最夸张的形象成为“真实”，其脑子里也没有“不可能”的概念。</p><p><strong>8. 最能打动群体的一般都是最假的事：</strong> 群体只能进行形象思维，也只有形象能够吓住或者吸引住它，从而驱动它。</p><p><strong>9. 领导者操纵群体主要靠想象：</strong> 所有重大的历史现象，都是群体的想象收到强烈的直接或间接的结果。可以说，掌握了激活群体想象的艺术，就掌握了统治和操纵它们的艺术。</p><h3 id="第四章：群体信念都采用一种宗教形式"><a href="#第四章：群体信念都采用一种宗教形式" class="headerlink" title="第四章：群体信念都采用一种宗教形式"></a>第四章：群体信念都采用一种宗教形式</h3><p><strong>1. 宗教热忱：</strong> 群体中，好感会迅速变成崇拜，讨厌瞬间变成仇恨；群体的信念正是基于这个条件而形成一种特殊性形式，称为宗教热忱。</p><p><strong>2. 宗教热忱的特点：</strong> 崇拜某种高级的存在，赋予这种存在某些力量并敬畏它，盲目服从它的要求，无法质疑其信条，渴望扩散这些信条，倾向于仇视任何不信的人、排异且狂热。</p><p><strong>3. 信徒：</strong> 信徒不只是拜神的人，当一个人全心、全意、全灵魂地顺从，把心、意、灵魂狂热的风险给一项事业或者一个人，将其作为自己思想、行为的指导和归宿，他就是一个信徒。</p><p><strong>4. 群体需要神，甚于其他一切：</strong> 群体在热忱中总是下意识地把神秘的力量赋予它们的政治领袖；而一切政治、鬼神和社会理念只有采取了宗教形式才能深入人心，因为它能让人沉默从而排除危险；由此，无神论也采取宗教的形式。</p><p><strong>5. 只有群众的宗教热忱才能引起大革命、大动荡、大屠杀</strong>，而最专制的暴君和最绝对的权力也只能加速或延缓群众的热忱，而无法干涉其中。</p><h2 id="第二卷：群体的思想观念"><a href="#第二卷：群体的思想观念" class="headerlink" title="第二卷：群体的思想观念"></a>第二卷：群体的思想观念</h2><h3 id="第一章：群体观念的间接因素"><a href="#第一章：群体观念的间接因素" class="headerlink" title="第一章：群体观念的间接因素"></a>第一章：群体观念的间接因素</h3><p><strong>1. 群体观念的直接因素和间接因素：</strong> 间接因素指让群众接受某些信念并对其他信念绝缘的因素，它长期酝酿在群体中，具有某些基础性，也是群体思想观念的前期准备性因素；直接因素指间接因素迎来高潮时敦促群体付诸行动的刺激因素，它使观念释放结果并变成行动。</p><p><strong>2. 间接因素1，民族因素：</strong> 最重要因素；一切文明成分都是民族性格的外在表现，当下环境只会暂时改变民族性格的表现；各民族在思想行为方面存在巨大差异。</p><p><strong>3. 间接因素2，传统因素：</strong> 传统代表过去的思想感情和需要，是民族性的综合反映；传统支配群体，人们也尽力改变传统，只是只能改变传统的名称和外形；没有传统，民族性和文明都将不复存在。</p><p><strong>4. 人类的两大任务：</strong> 建立传统结构和在它的益处消失后尽力摧毁它。</p><p><strong>5. 间接因素3，时间因素：</strong> 时间形成了群体的思想观念、积攒无数思想观念的碎片，是某个时代的理念能够生长；它是唯一的创造者，也是唯一的毁灭者。</p><p><strong>6. 间接因素4，社会政治制度：</strong> 制度是思想感情和传统的产物，它并不创造时代，而是被时代所创造；各种制度没有本质上的优劣，因为一个国家没有真正改变自己的制度的能力；暴力革命无疑可以改变制度名称，但是其本质不会变化；制度和一个民族的伟大或衰败没有关系。</p><p><strong>7. 所谓的民主：</strong> 个人的权力最受尊重，个人拥有最多的自由。</p><p><strong>8. 间接因素5，教育因素：</strong> 教育即可以改造人、提升人、促进人人平等，也可以将群体恶化走向极端。</p><h3 id="第二章：群体思想的直接因素"><a href="#第二章：群体思想的直接因素" class="headerlink" title="第二章：群体思想的直接因素"></a>第二章：群体思想的直接因素</h3><p><strong>1. 直接因素1，关键词、套话和形象：</strong> 形象都不是用手塑造的，利用关键词加套话可以巧妙地将它们激活；关键词激活的形象会产生力量，而这和它真正的含义毫无关系，有时词越笼统、越囊括各种渴望而高深越有力；套话一成不变，特定的词语携带特定的时代形象；但是关键词加套话在用了过多或受到冲击后会渐渐失去作用，因而，政治家最根本的任务之一就是用新的关键词加套话给民众烙印新的形象。</p><p><strong>2. 直接因素2，幻想：</strong> 文明出现以来，所有的政治、宗教、艺术和社会理念都被幻想所掌控；它既是虚假的影子，也推动创造最伟大艺术和文明；哲学创造幻想，为群众提供理想；科学太过理性，无法承诺太多也不会撒谎。</p><p><strong>3. 直接因素3，</strong> 经验教训：经验教训让真理在群众心理扎根，摧毁膨胀的太危险的幻想。</p><p><strong>4. 直接因素4，理性：</strong> 群体不受理想影响；要影响群体，要先弄清楚群体产生情绪的原因，并假装自己也有这种情绪，然后用直线思维唤起强烈的暗示，去修改群体的情绪；民族精神结构中自带着本民族的宿命，它是一种无法被遏制的冲动，显然不受理性控制；创造文明的主因是感情不是理性，这包括荣耀、牺牲、信仰、爱国以及对光荣的热爱。</p><h3 id="第三章：群体领袖及其掌控方式"><a href="#第三章：群体领袖及其掌控方式" class="headerlink" title="第三章：群体领袖及其掌控方式"></a>第三章：群体领袖及其掌控方式</h3><p><strong>1. 群体领袖：</strong> 一切生物都有服从领袖的本能，没有领袖就无法完成任何事情；领袖一开始常是被领导者，他先被理论完全占据，放弃自己的一切，然后认为一切不符合该理论的思想都是异端；他们通常是实干家、天生鲁莽、病态的坚信、高度亢奋、信念坚定，完全是梦想的奴隶；他们的功能是激发信仰，只要个体一进入群体就会迅速受到领袖的影响；大部分人并不对任何问题有任何清晰的理性的观念，领袖指导他们；群体领袖都很专权，而专制是拥有追随者的前提。</p><p><strong>2. 人类任何可是使用的力量中，信念的力量最为强大。</strong></p><p><strong>3. 领袖的掌控方式1，断言：</strong> ​不带任何逻辑的、简练纯粹的断言是洗脑最有效的方法之一；断言越简练，越不带任何论据和论证，就越有威力。</p><p><strong>4. 领袖的掌控方式2，重复：</strong> 反复的断言会使信息以特别的方式深入人心，并最终被接受为不证自明的真理。</p><p><strong>5. 领袖的掌控方式3，传染：</strong> 当断言经过充分的重复，强大的传染机制就会运行；群体的各种思想观念、感情情绪都有强烈的传染性，而传染性会让所有人产生统一的倾向；很多社会现象都可以归因为模仿，而模仿不过是传染性的结果之一。</p><p><strong>6. 气场：</strong> 气场是一个人、一本书或者一种理念的精神控制力，这种控制力会完全麻痹我们的判断力，它是一切权威感的主要支撑力量。</p><p><strong>7. 外来气场：</strong> 一个人有钱、有身份、有地位可以使他有强大的气场，即便他本人毫无价值；各种文学著作、理论、艺术作品则可以在长期重复后拥有类似的气场麻痹群体。</p><p><strong>8. 内在气场：</strong> 只被一小部分人所拥有，他们可以是周围人无条件接受自己的思想感情，这种人通常是最伟大的群众领袖，如佛陀、耶稣、穆罕默德；这种气场总是无意识的，但是几乎完美，导致那个时代的人不自觉就接受了他们的思想。</p><p><strong>9. 气场也会消失：</strong> 它随成功而逝，也在争议中被消磨；因而，任何长期保持气场的神和人，都不允许自己成为议题，为了让群众敬畏，必须与其保持距离。</p><h3 id="第四章：群体思想观念的变化极限"><a href="#第四章：群体思想观念的变化极限" class="headerlink" title="第四章：群体思想观念的变化极限"></a>第四章：群体思想观念的变化极限</h3><p><strong>1. 牢固的信念：</strong> 某些长久不变的信念，它们决定一个文明的走向，但是数量有限，仅仅是文明的骨架；建立和消除都十分困难；因为共同信念，每个时代的所有人都被网进了一套传统、观念、习俗中，无法摆脱束缚，所有人都很像。</p><p><strong>2. 信念的消失：</strong> 当它的价值被质疑的时候，任何伟大的信念都开始消失；任何信念都是一种虚构，只有在不细究的前提下才能存在。</p><p><strong>3. 对人的潜意识的统治是唯一真正的暴政，因为人们无法反抗。</strong></p><p><strong>4. 可变的观念：</strong> 如果一种理念与民族的共同信念毫无关系，它就受制于偶然性，是暗示和传染形成的观念，是动态的，迅速产生和消失。</p><p><strong>5. 可变观念产生的原因1，古老信念的失势：</strong> 共同信念式微，民众需要新的信念。</p><p><strong>6. 可变观念产生的原因2，民众力量的不断增强：</strong> 它们极度不稳定，因为没有约束而更加放纵。</p><p><strong>7. 可变观念产生的原因3，媒体的快速发展：</strong> 记者带来相反的思想，而每种观点带来的信息迅速被相反的信息抵消掉，而导致任何观点都不能普及而成为共同信念。</p><p><strong>8. 可变观念产生的结果：</strong> 政府无力主导民众思想；政客为了迎合民意，方针反复无常；不受理想只受感情支配的民意越来越成为政治的最高指导思想；民众越来越没有思想，越来越高度动态，也越来越麻木。</p><p><strong>9. 哲学家的任务：</strong> 研究便面不同的观念背后的古老信念，并在思想变迁中找到由民族性格和共同信念决定的那部分信念。</p><h2 id="第三卷：群体的分类及各种群体的特点"><a href="#第三卷：群体的分类及各种群体的特点" class="headerlink" title="第三卷：群体的分类及各种群体的特点"></a>第三卷：群体的分类及各种群体的特点</h2><h3 id="第一章：群体的分类"><a href="#第一章：群体的分类" class="headerlink" title="第一章：群体的分类"></a>第一章：群体的分类</h3><p><strong>1. 分类1，异质性群体：</strong> 本书一直在研究的群体，有各种职业、才智很分散的人组成；又分为：匿名群体和实名群体。</p><p><strong>2. 异质性群体从可以分为不同民族：</strong> 民族性对群体性格发挥全面的影响，十分强大，是主要因素；群体性格属于次要因素。</p><p><strong>3. 分类2，同质性群体：</strong> 有诸多相同属性；又分为 <strong>党派：</strong> 职业、文化大不相同，但是被相同信念凝聚、 <strong>阶层：</strong> 从业、文化相似、社会地位相当，和 <strong>阶级：</strong> 靠经济利益、生活习惯和文化程度几乎完全相同而凝聚。</p><h3 id="第二章：所谓犯罪群体（异质性匿名群体）"><a href="#第二章：所谓犯罪群体（异质性匿名群体）" class="headerlink" title="第二章：所谓犯罪群体（异质性匿名群体）"></a>第二章：所谓犯罪群体（异质性匿名群体）</h3><p><strong>1. 犯罪群体依法成立，但心理学上不成立：</strong> 群体会完全自动进入无意识状态，易被操控，因而它们无法抵挡怂恿；它们有所有群体共有的特点：易受暗示性、轻信性、不稳定性、感情夸张性、某些高尚性。</p><p><strong>2. 群体的善良和残忍一样极端：</strong> 有正直和原始的正义感，当它们幼稚的良知得到了满足，就会释放和善良一样极端的残忍和杀戮本性。</p><h3 id="第三章：刑事陪审团（异质性实名群体）"><a href="#第三章：刑事陪审团（异质性实名群体）" class="headerlink" title="第三章：刑事陪审团（异质性实名群体）"></a>第三章：刑事陪审团（异质性实名群体）</h3><p><strong>1. 总特点：</strong> 易受暗示性、缺乏理性、容易受群体领袖的影响、受无意识情绪掌控，群体成员的才智水平没什么用。</p><p><strong>2. 陪审团收情绪掌控，极少被证据打动：</strong> 好律师关心怎么影响陪审团的心情，极少讲理，只使用低级逻辑模式（缺乏理性、受无意识情绪掌控）；律师不需要改变所有陪审员的观点，只要控制那些少数指导整个群体的灵魂人物即可（易受暗示、易受群体领袖影响）。</p><p><strong>3. 陪审团制度的意义：</strong> 尽管陪审团制度缺陷重重，但面对一个实际上常犯错误又没有制衡的阶层，陪审团制度是我们保护自己免受其害的唯一方法。</p><h3 id="第四章：投票群体（异质性匿名群体）"><a href="#第四章：投票群体（异质性匿名群体）" class="headerlink" title="第四章：投票群体（异质性匿名群体）"></a>第四章：投票群体（异质性匿名群体）</h3><p><strong>1. 投票群体：</strong> 有权选人任职的群体；行为仅限于选择候选人这一件事，因而，它们只表现出群体特点的一部分。</p><p><strong>2. 投票群体的特点：</strong> 理性匮乏、判断力丧失 、易怒、轻信、简单、已被领袖影响。</p><p><strong>3. 拉票方法：</strong> 候选人需要被群众接受自己，这需要有 <strong>气场</strong> ，如果内在气场不够，则需要用金钱等弥补外在气场；候选人需要用最离谱的奉承去淹没投票群体，做出最异想天开的承诺时必须要毫不犹豫；候选人的书面纲领不能太具体，但口头承诺多夸张都不过分；反复断言自己的观点，反复辱骂对手的观点，但是不要带任何逻辑；善用关键词加套话。</p><p><strong>4. 委员会也许是群体力量带来的最可怕的危险：</strong> 它实际上最非人化，因而是最具压迫性的专制体制；领袖指导委员会，其言行都被认为是以群体名义做出的，所以没有任何责任约束，处于这个位置的人可以为所欲为。</p><p><strong>5. 民族性格：</strong> 在任何国家，当选者的普遍观念代表着民族性格，一代代不会有什么明显的不同；民族是首要因素，它派生政府；制度只是很小的影响因素； <strong>民族性及日常生活的负累，是主宰我们命运的两大神秘因素</strong> 。</p><h3 id="第五章：议会（异质性实名群体）"><a href="#第五章：议会（异质性实名群体）" class="headerlink" title="第五章：议会（异质性实名群体）"></a>第五章：议会（异质性实名群体）</h3><p><strong>1. 议会：</strong> 虽然各国在不同时期有不同的议员选举方式，但在治理、辩论和表决方面有诸多相同的困难；虽然议会制是许多现代文明国家的理想，但从心理学角度讲，它表达了一种普遍错误的信念，即一群人比少数人更能做出明智而公正的决断。</p><p><strong>2. 议会的特点：</strong> 低智商化、易怒、易受暗示、感情夸张、少数领袖操控、简单幼稚；因为它们成员的特殊，在一些方面有与普通群体不同。</p><p><strong>3. 易受暗示性有非常明确的底线：</strong> 在 <strong>地方性法规和部门规章上</strong> ，议会成员意见坚定；在 <strong>一般性事务上</strong> ，它们既有主见又无主见，总在两方意见之间犹豫，既被领袖操控又受选民影响。</p><p><strong>4. 领袖必须存在：</strong> 其实是他们在统治议会；群体没有主人不行，所以一个代表团的决定一般只是代表少数人的意见。</p><p><strong>5. 偶尔有聪明领袖：</strong> 他们受过高等教育，聪慧，追求理性和逻辑，但是这对他们有害；理性使得他们将事态说得复杂；逻辑让他们的信念允许解释，请求理解，严重缺乏信徒所必备的狂热信念，所以他们显得软弱和愚蠢。</p><p><strong>6. 议会的意义：</strong> 虽然议会问题丛生，但它仍然是人类至今发明的最佳治国方式，尤其是人类发现的摆脱个人专制的最佳方式。</p><p><strong>7. 议会的危险：</strong> 不可避免的财政浪费、对个人自由的不断压缩。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><h3 id="文明如何演变？"><a href="#文明如何演变？" class="headerlink" title="文明如何演变？"></a>文明如何演变？</h3><p><strong>1. 文明之初：</strong> 一群人来源不同的人汇聚一处，是头领制定规矩，但人们并不完全服从，出现了短暂的心里群体特征，短暂团结、英勇、冲动。</p><p><strong>2. 时间开始作用：</strong> 共生的各民族之间不断融合，开始拥有共同的特征和感情，遗传则加固它们。</p><p><strong>3. 新文明的诞生：</strong> 通过长期的努力、反复的斗争和无处次重头再来后，他们获得了某种共同的理想，产生了新的文明，并产生了相应的制度、信念和艺术；在这背后，民族性格产生了，它把这个民族的变化限定在很小的范围内，限制偶然性的作用。</p><p><strong>4. 文明开始衰微：</strong> 时间在完成创作之后，便开始了毁灭；在共同理想开始衰微时，文明便开始衰落；民族日益丧失自我增长和自我凝聚的力量；个人的个性和智力会提高，但民族的集体自我意识却会被膨胀的个体自我意识所取代，同时个体的人格和行动力加强；在这个阶段，虽然传统和制度还表面连接着个体，但被不同的利益和诉求拆散的人们不再有自我管理能力，连最小的事务也需要国家的指导，于是国家开始发挥支配作用。</p><p><strong>5. 民族性格完全接解体：</strong> 当古老的信念完全消息，民族变成了单独的个体，复归原始状态——群体；它极不稳定也没有未来，只拥有群体所拥有的动态性和偶然性；这时候文明也许看起来还耀眼，因为古老的历史造就的外边还在，但实际它就是一座正在坍塌的大厦。</p><p><strong>6. 周而复始：</strong> 为了追求理想，从野蛮走向文明；当理想失去了价值，从文明走向衰落和灭亡。这就是一个民族的生命的循环过程。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Book Notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>First Step on Hexo</title>
    <link href="/First%20Step%20on%20Git%20Hexo/"/>
    <url>/First%20Step%20on%20Git%20Hexo/</url>
    
    <content type="html"><![CDATA[<p><a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> is a static blog framework, based on Node.js. This post is written for backupping installation procedue for Windows OS.</p><h2 id="Content"><a href="#Content" class="headerlink" title="Content:"></a>Content:</h2><ul><li>Install <a href="https://nodejs.org/zh-cn/download/" target="_blank" rel="noopener">Node.js</a></li><li>Install <a href="https://www.github.com" target="_blank" rel="noopener">Git</a></li><li>Sign in <a href="github.com">Github</a></li><li>Install <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a></li><li>Deploy local source to Github</li><li>Write, review, and publish posts</li><li>Customize domain name</li><li>Customize blog outlook (<a href="https://github.com/hpcslag/hexo-theme-morgan" target="_blank" rel="noopener">Morgan</a> Theme)</li><li>FAQ</li></ul><h2 id="Install-Node-js"><a href="#Install-Node-js" class="headerlink" title="Install Node.js"></a>Install <a href="https://nodejs.org/zh-cn/download/" target="_blank" rel="noopener">Node.js</a></h2><p>Download LTS version for Windows(.msi), and install as default (click <code>Next</code> all the way alone).</p><p>Use <code>Win+R</code> and enter <code>cmd</code> to open MS-DOS. Enter <code>node -v</code> and <code>npm -v</code> checking whether Node.js has been set correctly.</p><h2 id="Install-Git"><a href="#Install-Git" class="headerlink" title="Install Git"></a>Install <a href="https://www.github.com" target="_blank" rel="noopener">Git</a></h2><p>Please visit <a href="https://ingingx.xyz/Git-Installation-and-Badic-Git-Comments/" target="_blank" rel="noopener">here</a> for Git installation, but choose <code>Use Git from the Windows Command Prompt</code> at the last step. </p><p>Enter <code>git --version</code> checking whether Git has been set correctly.</p>]]></content>
    
    
    
    <tags>
      
      <tag>draft</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>git 代理配置与 git clone 下载加速</title>
    <link href="/git%20clone%20%E4%BB%A3%E7%90%86%E4%B8%8E%E5%8A%A0%E9%80%9F/"/>
    <url>/git%20clone%20%E4%BB%A3%E7%90%86%E4%B8%8E%E5%8A%A0%E9%80%9F/</url>
    
    <content type="html"><![CDATA[<p>Notes: 此文将介绍两种方法解决墙内 <code>git clone</code> 下载慢的问题。其中方法一在本文发出时（2020-02-26）测试无效，但去年（2019年）测试有效；方法二需要使用 ssr 或 v2ray 代理，请尝试前确定您可以在使用代理的情况下打开<a href="www.google.com">谷歌搜索</a>。</p><h2 id="1-修改-host-文件以加速-git-clone-和-GitHub-访问"><a href="#1-修改-host-文件以加速-git-clone-和-GitHub-访问" class="headerlink" title="1. 修改 host 文件以加速 git clone 和 GitHub 访问"></a>1. 修改 <code>host</code> 文件以加速 <code>git clone</code> 和 GitHub 访问</h2><h3 id="steps"><a href="#steps" class="headerlink" title="steps:"></a>steps:</h3><p>在 <a href="https://www.ipip.net/ip.html" target="_blank" rel="noopener">ip 查询网页</a> 查询以下几个网址的当前 IP 地址：</p><pre class=" language-hljs plain"><code class="language-hljs plain">github.global.ssl.fastly.Netgithub.com</code></pre><p>其中，第一个在 GitHub 上 clone 时的下载服务器地址，第二个是 GitHub 官网地址。</p><p>由于本地 IP 不同，所以每个人查询到的 GitHub 访问 IP 也不同。我查询出来是：</p><pre class=" language-hljs plain"><code class="language-hljs plain">199.59.148.14013.229.188.59</code></pre><p>记录下这几个 IP， 去 host 文件，右键使用记事本打开：C:\Windows\System32\drivers\etc\hosts</p><p>在 host 文件末尾添加：</p><pre class=" language-hljs plain"><code class="language-hljs plain">github.global.ssl.fastly.Net 199.59.148.140github.com 13.229.188.59</code></pre><p>最后使用命令提示符刷新 DNS 缓存，让计算机每次访问以上两个网址的时候都是用我们设定的 IP 访问 （这样理论上就可以避免被墙）：</p><pre class=" language-hljs bash"><code class="language-hljs bash">$ ipconfig /flushdns</code></pre><h3 id="notes"><a href="#notes" class="headerlink" title="notes:"></a>notes:</h3><p>之前博主也是使用这种办法加速 <code>git clone</code>，但是最近似乎墙更高了，在两台电脑上测试了多次都无法加速。如果有大佬知道其中缘由或者可以改进还望不吝赐教。</p><h2 id="2-通过修改-Git-本身的配置以加速-git-clone"><a href="#2-通过修改-Git-本身的配置以加速-git-clone" class="headerlink" title="2. 通过修改 Git 本身的配置以加速 git clone"></a>2. 通过修改 Git 本身的配置以加速 <code>git clone</code></h2><h3 id="steps-1"><a href="#steps-1" class="headerlink" title="steps:"></a>steps:</h3><p>连接你的 vpn，设置为“全局代理模式”。可以通过尝试打开<a href="www.google.com">谷歌搜索</a>来测试是否连接成功。</p><p>找到 vpn 的本地代理端口（一般是1080，但是也可能是其他的），我这里使用的是1080。</p><p>打开命令提示符，查看 Git 的的配置：</p><pre class=" language-hljs bash"><code class="language-hljs bash">$ git config -l</code></pre><p>确认你的配置列表里没有 <code>https.proxy</code>，<code>http.proxy</code> 等带有 <code>proxy</code> 的配置。如果有，请参照本文下一部分（3）中的“取消代理设置”将之取消。</p><p>如果你使用 ssr 代理（socks5 协议），则输入：</p><pre class=" language-hljs bash"><code class="language-hljs bash">$ git config --global http.proxy socks5://127.0.0.1:1080$ git config --global https.proxy socks5://127.0.0.1:1080</code></pre><p>配置所有 Git 的网络访问都通过代理完成；</p><p>如果你使用 v2ray 代理（http/https 协议），则输入：</p><pre class=" language-hljs bash"><code class="language-hljs bash">$ git config --global http.proxy http://127.0.0.1:1080$ git config --global https.proxy https://127.0.0.1:1080</code></pre><h3 id="notes-1"><a href="#notes-1" class="headerlink" title="notes:"></a>notes:</h3><p>这个方法需要使用 ssr 或 v2ray 等 vpn 代理，这通常需要付费租用代理服务器或者购买节点。但好处是比较稳定，只要代理没有问题，Git 就可以满带宽下载（unlike 方法1，花钱才能变强）。博主目前使用了3个月，没有出现任何问题。</p><h2 id="3-Git-的代理配置和一些思考"><a href="#3-Git-的代理配置和一些思考" class="headerlink" title="3. Git 的代理配置和一些思考"></a>3. Git 的代理配置和一些思考</h2><p>在命令提示符，可以查看 Git 的的配置：</p><pre class=" language-hljs bash"><code class="language-hljs bash">$ git config -l</code></pre><p><img src="/image/gitConfig.png" srcset="/img/loading.gif" alt=""></p><p>其中，最后两行就是我们刚设置的代理。</p><p>如果想要恢复某个设置为初始值，可以这样：</p><pre class=" language-hljs bash">$ git config --global --<span class="hljs-built_in"><code class="language-hljs bash">$ git config --global --<span class="hljs-built_in">unset</span> [configName]</code></pre><p>比如，想要取消刚才设置的代理：</p><pre class=" language-hljs bash">$ git config --global --<span class="hljs-built_in"><code class="language-hljs bash">$ git config --global --<span class="hljs-built_in">unset</span> http.proxy$ git config --global --<span class="hljs-built_in">unset</span> https.proxy</code></pre><p>看一下 <code>git config -l</code>，是不是就没有代理的那个变量了？</p><h3 id="notes-2"><a href="#notes-2" class="headerlink" title="notes:"></a>notes:</h3><p>Git 是强大的工具，想用好它绝不是几个命令就ok的。通常使用的命令大约有10-20个，要精通则需要熟练至少50个相关的命令以及搞懂各个部分的功能和组成结构。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Start with Git</title>
    <link href="/Start%20with%20Git/"/>
    <url>/Start%20with%20Git/</url>
    
    <content type="html"><![CDATA[<p><a href="https://git-scm.com/" target="_blank" rel="noopener">Git</a> is a free and open source distributed version control system. We can use it to operate our <a href="https://www.github.com" target="_blank" rel="noopener">GitHub</a> repos. Now let’s see its installation and some basic comments!</p><h2 id="Part-1-Git-Installation-for-Windows"><a href="#Part-1-Git-Installation-for-Windows" class="headerlink" title="Part 1: Git Installation (for Windows)"></a>Part 1: Git Installation (for Windows)</h2><h3 id="1-Download-‘git-exe’"><a href="#1-Download-‘git-exe’" class="headerlink" title="1. Download ‘git.exe’"></a>1. Download ‘git.exe’</h3><p>Click <a href="https://git-scm.com/download/win" target="_blank" rel="noopener">here</a> and download the right version.</p><p>You may need some proxy to accelerate the downloading process.</p><h3 id="2-Click-‘next’"><a href="#2-Click-‘next’" class="headerlink" title="2. Click ‘next’"></a>2. Click ‘next’</h3><p>Open your ‘.exe’ file just downloaded, and click ‘next’ for default installation, or choose ‘custom install’ if you know what you are doing.</p><h3 id="3-Now-it’s-all-set-Try-out-your-first-Git-Comment"><a href="#3-Now-it’s-all-set-Try-out-your-first-Git-Comment" class="headerlink" title="3. Now it’s all set! Try out your first Git Comment!"></a>3. Now it’s all set! Try out your first Git Comment!</h3><p>See Part 2 below! </p><h2 id="Part-2-Basic-Git-Comments"><a href="#Part-2-Basic-Git-Comments" class="headerlink" title="Part 2: Basic Git Comments"></a>Part 2: Basic Git Comments</h2><h3 id="1-Git-Architecture"><a href="#1-Git-Architecture" class="headerlink" title="1. Git Architecture"></a>1. Git Architecture</h3><p><img src="/image/gitComments.png" srcset="/img/loading.gif" alt=""></p><h3 id="2-Basic-Git-Comments"><a href="#2-Basic-Git-Comments" class="headerlink" title="2. Basic Git Comments"></a>2. Basic Git Comments</h3><pre class=" language-hljs bash"><span class="hljs-comment"><code class="language-hljs bash"><span class="hljs-comment"># initial current folder</span>$ git init <span class="hljs-comment"># clone / dowmload a repo</span>$ git <span class="hljs-built_in">clone</span> [url] [givenDir]<span class="hljs-comment"># show Git Configure File</span>$ git cinfig -l <span class="hljs-comment"># set Username and Password</span>$ git config --global user.name <span class="hljs-string">"yourUsername"</span>$ git config --global user.email <span class="hljs-string">"yourEmail"</span>$ git config --global user.password <span class="hljs-string">"yourPassword"</span><span class="hljs-comment"># add file from 'givenDir' to 'Index'</span>$ git add [givenDir]<span class="hljs-comment"># add file from 'currentDir' to 'Index'</span>$ git add .<span class="hljs-comment"># rename and add file to 'Index'</span>$ git mv [file-original] [file-renamed]<span class="hljs-comment"># submit 'Index' to 'Repo'</span>$ git commit -m [message]<span class="hljs-comment"># submit 'Workspace' to 'Repo'</span>$ git commit -a<span class="hljs-comment"># show file changes</span>$ git status<span class="hljs-comment"># show current-branch changes</span>$ git <span class="hljs-built_in">log</span><span class="hljs-comment"># show difference between 'Workspace' and 'Index'</span>$ git diff<span class="hljs-comment"># show all the 'Remote' Repos' info</span>$ git remote -v<span class="hljs-comment"># show ONE 'Remote' Repo's info</span>$ git remote show [remote]<span class="hljs-comment"># fetch all 'Remote' Repos and merge into 'Workspace'</span>$ git fetch [remote]<span class="hljs-comment"># fetch 'Remote' changes, and merge to local branches</span>$ git pull [remote] [branch]<span class="hljs-comment"># push all 'Repos' to 'Remote'</span>$ git push [remote] --all<span class="hljs-comment"># push current 'Repo' to 'Remote', ignoring conflict</span>$ git push [remote] --force<span class="hljs-comment"># release a downloadable archive file</span>$ git archive</code></pre><h3 id="3-Some-words"><a href="#3-Some-words" class="headerlink" title="3. Some words"></a>3. Some words</h3><p>Of course, this article is just a small glance of Git. As referred in the beginning, Git is a powerful tool. You can read <a href="https://git-scm.com/docs" target="_blank" rel="noopener">official documents</a> for more detailed and advanced usage. </p><p>And at the same time, I will keep this article updated along with my journey of coding and studying.</p>]]></content>
    
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/Hello%20world/"/>
    <url>/Hello%20world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-hljs bash">$ hexo new <span class="hljs-string"><code class="language-hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-hljs bash"><code class="language-hljs bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-hljs bash"><code class="language-hljs bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-hljs bash"><code class="language-hljs bash">$ hexo deploy</code></pre><h3 id="中文测试"><a href="#中文测试" class="headerlink" title="中文测试"></a>中文测试</h3><pre class=" language-hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">helper</span><span class="hljs-params"><code class="language-hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">helper</span><span class="hljs-params">(str s1, str s2)</span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
